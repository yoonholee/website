% Encoding: UTF-8

@article{lee2020neural,
    title={Neural Complexity Measures},
    author={Yoonho Lee and Juho Lee and Sung Ju Hwang and Eunho Yang and Seungjin Choi},
    year={2020},
    month = {8},
    paper={https://arxiv.org/abs/2008.02953},
    abstract={While various complexity measures for diverse model classes have been proposed, specifying an appropriate measure capable of predicting and explaining generalization in deep networks has proven to be challenging. We propose \textit{Neural Complexity} (NC), an alternative data-driven approach that meta-learns a scalar complexity measure through interactions with a large number of heterogeneous tasks. The trained NC model can be added to the standard training loss to regularize any task learner under standard learning frameworks. We contrast NC's approach against existing manually-designed complexity measures and also against other meta-learning models, and validate NC's performance on multiple regression and classification tasks. }
}

@article{lee2020bootstrapping,
    title={Bootstrapping Neural Processes},
    author={Juho Lee* and Yoonho Lee* and Jungtaek Kim and Eunho Yang and Sung Ju Hwang and Yee Whye Teh},
    year={2020},
    month = {8},
    paper={https://arxiv.org/abs/2008.02956},
    abstract={Unlike in the traditional statistical modeling for which a user typically hand-specify a prior, Neural Processes (NPs) implicitly define a broad class of stochastic processes with neural networks. Given a data stream, NP learns a stochastic process that best describes the data. While this "data-driven" way of learning stochastic processes has proven to handle various types of data, NPs still relies on an assumption that uncertainty in stochastic processes is modeled by a single latent variable, which potentially limits the flexibility. To this end, we propose the Bootstrapping Neural Process (BNP), a novel extension of the NP family using the bootstrap. The bootstrap is a classical data-driven technique for estimating uncertainty, which allows BNP to learn the stochasticity in NPs without assuming a particular form. We demonstrate the efficacy of BNP on various types of data and its robustness in the presence of model-data mismatch.}
}

@article{seo2020penultimate,
  title={On the Distribution of Penultimate Activations of Classification Networks},
  author={Seo*, Minkyo and Lee*, Yoonho and Kwak, Suha},
  year={2020},
  month = {2},
  abstract={This paper studies probability distributions of penultimate activations of deep classification networks. We first identify that learning a classification network with the cross-entropy loss makes its (normalized) penultimate activations follow a von Mises-Fisher distribution for each class, which is parameterized by weights of its final fully-connected layer. Through this analysis, we derive a probability density function of penultimate activations per class. This generative model allows to synthesize activations of classification networks without feeding images forward through them. We also demonstrate through experiments that our generative model of penultimate activations can be used for knowledge distillation and class-conditional image generation.}
}

@article{lee2019discrete,
  title={Discrete Infomax Codes for Supervised Representation Learning},
  author={Lee, Yoonho and Kim, Wonjae and Park, Wonpyo and Choi, Seungjin},
  journal={arXiv:1905.11656},
  year={2019},
  month = {10},
  paper={https://arxiv.org/abs/1905.11656},
  abstract={Learning compact discrete representations of data is a key task on its own or for facilitating subsequent processing of data. In this paper we present a model that produces Discrete InfoMax COdes (DIMCO); we learn a probabilistic encoder that yields k-way d-dimensional codes associated with input data. Our model’s learning objective is to maximize the mutual information between codes and labels with a regularization, which enforces entries of a codeword to be as independent as possible. We show that the infomax principle also justiﬁes previous loss functions (e.g., cross-entropy) as its special cases. Our analysis also shows that using shorter codes, as DIMCO does, reduces overﬁtting in the context of few-shot classiﬁcation. Through experiments in various domains, we observe this implicit meta-regularization effect of DIMCO. Furthermore, we show that the codes learned by DIMCO are efﬁcient in terms of both memory and retrieval time compared to previous methods.},
  selected={yes}
}

@Article{lee2019deep,
  abbr={NeurIPS-W},
  author        = {Juho Lee and Yoonho Lee and Yee Whye Teh},
  title         = {Deep Amortized Clustering},
  year          = {2019},
  month = {9},
  journal = {Sets and Parts Workshop @ NeurIPS 2019 (oral)},
  paper={https://arxiv.org/abs/1909.13433},
  abstract={We propose Deep Amortized Clustering (DAC), a framework in which a neural network learns to cluster datasets efficiently using a few forward passes through a deep neural network. DAC implicitly learns what makes a cluster, how to group data points into clusters, and how to count the number of clusters in datasets. DAC is meta-learned in a data-driven way, using only clustered datasets and their partitions. This framework differs from traditional clustering algorithms, which usually require user-specified prior knowledge about the shape or structure of clusters. We empirically show on both synthetic and image data that \gls{dac} can efficiently and accurately cluster novel datasets.}
}

@article{kim2019learning,
  abbr={NeurIPS},
  title={Learning Dynamics of Attention: Human Prior for Interpretable Machine Reasoning},
  author={Kim, Wonjae and Lee, Yoonho},
  journal={NeurIPS 2019},
  year={2019},
  month = {6},
  paper={https://arxiv.org/abs/1905.11666},
  code={https://github.com/kakao/DAFT},
  abstract={Without relevant human priors, neural networks may learn uninterpretable features. We propose Dynamics of Attention for Focus Transition (DAFT) as a human prior for machine reasoning. DAFT is a novel method that regularizes attention-based reasoning by modelling it as a continuous dynamical system using neural ordinary differential equations. As a proof of concept, we augment a state-of-the-art visual reasoning model with DAFT. Our experiments reveal that applying DAFT yields similar performance to the original model while using fewer reasoning steps, showing that it implicitly learns to skip unnecessary steps. We also propose a new metric, Total Length of Transition (TLT), which represents the effective reasoning step size by quantifying how much a given model's focus drifts while reasoning about a question. We show that adding DAFT results in lower TLT, demonstrating that our method indeed obeys the human prior towards shorter reasoning paths in addition to producing more interpretable attention maps.}
}

@Article{pmlr-v97-lee19d,
  abbr={ICML},
  author    = {Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam and Choi, Seungjin and Teh, Yee Whye},
  title     = {Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks},
  journal = {ICML 2019},
  year      = {2019},
  month = {2},
  paper = {http://proceedings.mlr.press/v97/lee19d/lee19d.pdf},
  code={https://github.com/juho-lee/set_transformer},
  abstract={Many machine learning tasks such as multiple instance learning, 3D shape recognition, and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces the computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating the state-of-the-art performance compared to recent methods for set-structured data.}
}

@Article{lee2018gradient,
  abbr={ICML},
  author  = {Lee, Yoonho and Choi, Seungjin},
  title   = {Gradient-based meta-learning with learned layerwise metric and subspace},
  journal = {ICML 2018},
  year    = {2018},
  paper={http://proceedings.mlr.press/v80/lee18a/lee18a.pdf},
  code={https://github.com/yoonholee/MT-net},
  abstract={
    Gradient-based meta-learning methods leverage gradient descent to learn the commonalities among various tasks. 
    While previous such methods have been successful in meta-learning tasks, they resort to simple gradient descent during meta-testing. 
    Our primary contribution is the MT-net, which enables the meta-learner to learn on each layer's activation space a subspace that the task-specific learner performs gradient descent on. 
    Additionally, a task-specific learner of an MT-net performs gradient descent with respect to a meta-learned distance metric, which warps the activation space to be more sensitive to task identity. 
    We demonstrate that the dimension of this learned subspace reflects the complexity of the task-specific learner's adaptation task, and also that our model is less sensitive to the choice of initial learning rates than previous gradient-based meta-learning methods. 
    Our method achieves state-of-the-art or comparable performance on few-shot classification and regression tasks.},
  selected={yes}
}

@Comment{jabref-meta: databaseType:bibtex;}
