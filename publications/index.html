<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Yoonho  Lee | Publications</title>
<meta name="description" content="Personal website
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link defer rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link defer rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link defer rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">

<!-- Styles -->
<link defer rel="stylesheet" href="/assets/css/main.css">
<link rel="shortcut icon" href="/assets/img/favicon.ico">

<!-- Font -->
<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@300;400;600;700&display=swap" rel="stylesheet">

<link rel="canonical" href="https://yoonholee.com/">
  </head>

  <body class=" 
    sticky-bottom-footer">

    <!-- Header -->

    <div id="nonFooter">

    <header>
  <!-- Nav Bar -->
  <nav
    id="navbar"
    class="navbar navbar-light bg-white navbar-expand-sm sticky-top"
  >
    <div class="container">
      
      <a
        class="navbar-brand title:"
        href="https://yoonholee.com/"
        >Yoonho Lee</a
      >
      <!-- Navbar Toogle -->
      <button
        class="navbar-toggler collapsed ml-auto"
        type="button"
        data-toggle="collapse"
        data-target="#navbarNav"
        aria-controls="navbarNav"
        aria-expanded="false"
        aria-label="Toggle navigation"
      >
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About 
            </a>
          </li>
          <!-- Other pages -->
                  
          <li class="nav-item active">
            <a class="nav-link" href="/publications/">
              Publications 
              <span class="sr-only">(current)</span>
              
            </a>
          </li>
               
          
          <!-- Blog -->
          <li
            class="nav-item "
          >
            <a class="nav-link" href="/blog/">
              Blog 
            </a>
          </li>
          
          <li class="nav-item">
            <a class="nav-link" href="/assets/pdf/CV.pdf">CV </a>
          </li>
        </ul>
      </div>
    </div>
  </nav>
</header>


    <!-- Content -->
    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <!-- <h1 class="post-title">Publications</h1> -->
    <p class="post-description"></p>
  </header>

  <article>
    <div class="publications">


  <div class="year">2021</div>
  <ol class="bibliography"><li><div class="row publication-row" id="lee2021risknet">
  <div class="col-sm-2 abbr">
    
  </div>

  <div class="col-sm-10">
    
    <span class="title"><b>Efficient Checkpoint-Based Meta-Learning With Gradient Matching</b></span>
    <span class="author">
         
      <b> Yoonho Lee</b>,         Juho Lee     
    </span>

    <span class="periodical">
      
      <em></em>
      
    </span>
    

    <span class="links">
       
      [<a class="abstract publink">abstract</a>] 
       
              
       
      [<a class="bib publink">bibtex</a>] 
       
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: RiskNet mimics the test loss both in terms of value and gradients.</strong> </p>
      <p>
    Meta-learning methods optimize components inside a learning algorithm to achieve low validation loss after task learning. Typically, such methods backpropagate through the entire learning procedure. Although such an approach can successfully learn to learn small tasks, they are fundamentally unable to scale to tasks involving large datasets or requiring long training runs. We propose a meta-learning method that does not suffer from such issues: we first save a set of model checkpoints obtained during regular task learning. Then, we use these checkpoints to train a separate neural network, which we call RiskNet, to mimic the risk functional’s value while also producing the same gradients for task learner parameters. We show that our gradient matching component is critical to sample efficiency and performance through experiments in few-shot regression and image classification benchmarks. Furthermore, the resulting model is shown to faithfully mimic the target gradients.
    </p>
    </span>
     
    
    <span class="bib hidden">
      <pre>@inproceedings{lee2021risknet,
  title     = {Efficient Checkpoint-Based Meta-Learning With Gradient Matching},
  author    = {Lee, Yoonho and Lee, Juho},
  year      = {2021}
}</pre>
    </span>
    
  </div>
</div>
</li>
<li><div class="row publication-row" id="nam2021diversity">
  <div class="col-sm-2 abbr">
    
  </div>

  <div class="col-sm-10">
    
    <span class="title"><b>Diversity Matters When Learning From Ensembles</b></span>
    <span class="author">
           Giung Nam,          Jongmin Yoon,        
      <b> Yoonho Lee</b>,         Juho Lee     
    </span>

    <span class="periodical">
      
      <em></em>
      
    </span>
    

    <span class="links">
       
      [<a class="abstract publink">abstract</a>] 
       
              
       
      [<a class="bib publink">bibtex</a>] 
       
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: Inputs promoting output diversities can be useful when distilling from deep ensembles.</strong> </p>
      <p>
    Deep ensembles excel in large-scale image classification tasks both in terms of prediction accuracy and calibration. Despite being simple to train, the computation and memory cost of deep ensembles limits their practicability. While some recent works propose to distill an ensemble model into a single model to reduce such costs, there is still a performance gap between the ensemble and distilled models. We propose a simple approach for reducing this gap, i.e., making the distilled performance close to the full ensemble. Our key assumption is that a distilled model should absorb as much function diversity inside the ensemble as possible. We first empirically show that the typical distillation procedure does not effectively transfer such diversity, especially for complex models that achieve near-zero training error. To fix this, we propose an augmentation-based distillation strategy that reveals diversity by seeking inputs for which ensemble member outputs disagree. We empirically show that a model distilled with such augmented samples indeed exhibits enhanced diversity, leading to improved performance.
    </p>
    </span>
     
    
    <span class="bib hidden">
      <pre>@inproceedings{nam2021diversity,
  title     = {Diversity Matters When Learning From Ensembles},
  author    = {Nam, Giung and Yoon, Jongmin and Lee, Yoonho and Lee, Juho},
  year      = {2021}
}</pre>
    </span>
    
  </div>
</div>
</li>
<li><div class="row publication-row" id="pakman2020ACP">
  <div class="col-sm-2 abbr">
    
  </div>

  <div class="col-sm-10">
    
    <span class="title"><b>Amortized Probabilistic Detection of Communities in Graphs</b></span>
    <span class="author">
           Yueqi Wang*,        
      <b> Yoonho Lee*</b>,         Pallab Basu,          Juho Lee,          Yee Whye Teh,          Liam Paninski,          Ari Pakman     
    </span>

    <span class="periodical">
      
      <em></em>
      
    </span>
    

    <span class="links">
       
      [<a class="abstract publink">abstract</a>] 
       
        [<a class="publink" href="https://arxiv.org/abs/2010.15727" target="_blank">paper</a>]         [<a class="publink" href="https://github.com/aripakman/attentive_clustering_processes" target="_blank">code</a>] 
       
      [<a class="bib publink">bibtex</a>] 
       
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: An attention-based method for probabilistically detecting communities within graphs.</strong> </p>
      <p>Learning community structures in graphs has broad applications across scientific domains. While graph neural networks (GNNs) have been successful in encoding graph structures, existing GNN-based methods for community detection are limited by requiring knowledge of the number of communities in advance, in addition to lacking a proper probabilistic formulation to handle uncertainty. We propose a simple framework for amortized community detection, which addresses both of these issues by combining the expressive power of GNNs with recent methods for amortized clustering. Our models consist of a graph representation backbone that extracts structural information and an amortized clustering network that naturally handles variable numbers of clusters. Both components combine into well-defined models of the posterior distribution of graph communities and are jointly optimized given labeled graphs. At inference time, the models yield parallel samples from the posterior of community labels, quantifying uncertainty in a principled way. We evaluate several models from our framework on synthetic and real datasets and demonstrate superior performance to previous methods. As a separate contribution, we extend recent amortized probabilistic clustering architectures by adding attention modules, which yield further improvements on community detection tasks.</p>
    </span>
     
    
    <span class="bib hidden">
      <pre>@article{pakman2020acp,
  title         = {Amortized Probabilistic Detection of Communities in Graphs},
  author        = {Ari Pakman and Yueqi Wang and Yoonho Lee and Pallab Basu and Juho Lee and Yee Whye Teh and Liam Paninski},
  year          = {2021},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  eprint        = {2010.15727}
}</pre>
    </span>
    
  </div>
</div>
</li>
<li><div class="row publication-row" id="seo2021ckd">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">UAI</abbr>
     
  </div>

  <div class="col-sm-10">
    
    <span class="title"><b>On the Distribution of Penultimate Activations of Classification Networks</b></span>
    <span class="author">
           Minkyo Seo*,        
      <b> Yoonho Lee*</b>,         Suha Kwak     
    </span>

    <span class="periodical">
      
      <em>UAI 2021</em>
      
    </span>
    

    <span class="links">
       
      [<a class="abstract publink">abstract</a>] 
       
        [<a class="publink" href="https://arxiv.org/abs/2107.01900" target="_blank">paper</a>]        
       
      [<a class="bib publink">bibtex</a>] 
       
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: Final FC layer weights contain information about class relations.</strong> </p>
      <p>
    This paper studies the probability distributions of penultimate activations of classification networks. Specifically, we show that, when a classification network is trained with the cross-entropy loss, its final classification layer forms a Generative-Discriminative pair with a generative classifier based on a specific distribution of penultimate activations. More importantly, the distribution is parameterized by the weights of the final fully-connected layer, and can be considered as a generative model that synthesizes the penultimate activations without feeding input data. We empirically demonstrate that this generative model enables stable knowledge distillation in the presence of domain shift, and can also transfer knowledge from a classifier to variational autoencoders and generative adversarial networks for class-conditional image generation.
    </p>
    </span>
     
    
    <span class="bib hidden">
      <pre>@inproceedings{seo2021ckd,
  title     = {On the Distribution of Penultimate Activations of Classification Networks},
  author    = {Seo*, Minkyo and Lee*, Yoonho and Kwak, Suha},
  booktitle = {Conference on Uncertainty in Artificial Intelligence (UAI)},
  year      = {2021}
}</pre>
    </span>
    
  </div>
</div>
</li></ol>

  <div class="year">2020</div>
  <ol class="bibliography"><li><div class="row publication-row" id="lee2020bootstrapping">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">NeurIPS</abbr>
     
  </div>

  <div class="col-sm-10">
    
    <span class="title"><b>Bootstrapping Neural Processes</b></span>
    <span class="author">
           Juho Lee*,        
      <b> Yoonho Lee*</b>,         Jungtaek Kim,          Eunho Yang,          Sung Ju Hwang,          Yee Whye Teh     
    </span>

    <span class="periodical">
      
      <em>NeurIPS 2020</em>
      
    </span>
    

    <span class="links">
       
      [<a class="abstract publink">abstract</a>] 
       
        [<a class="publink" href="https://arxiv.org/abs/2008.02956" target="_blank">paper</a>]    [<a class="publink" href="https://crossminds.ai/video/5fce48f0d78d8048ceb6e3be/" target="_blank">video</a>]       [<a class="publink" href="https://github.com/juho-lee/bnp" target="_blank">code</a>] 
       
      [<a class="bib publink">bibtex</a>] 
       
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: Improved uncertainty estimates in Neural Processes using bootstrapping.</strong> </p>
      <p>Unlike in the traditional statistical modeling for which a user typically hand-specify a prior, Neural Processes (NPs) implicitly define a broad class of stochastic processes with neural networks. Given a data stream, NP learns a stochastic process that best describes the data. While this "data-driven" way of learning stochastic processes has proven to handle various types of data, NPs still rely on an assumption that uncertainty in stochastic processes is modeled by a single latent variable, which potentially limits the flexibility. To this end, we propose the Boostrapping Neural Process (BNP), a novel extension of the NP family using the bootstrap. The bootstrap is a classical data-driven technique for estimating uncertainty, which allows BNP to learn the stochasticity in NPs without assuming a particular form. We demonstrate the efficacy of BNP on various types of data and its robustness in the presence of model-data mismatch.</p>
    </span>
     
    
    <span class="bib hidden">
      <pre>@inproceedings{lee2020bnp,
  title     = {Bootstrapping Neural Processes},
  author    = {Juho Lee* and Yoonho Lee* and Jungtaek Kim and Eunho Yang and Sung Ju Hwang and Yee Whye Teh},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2020}
}</pre>
    </span>
    
  </div>
</div>
</li>
<li><div class="row publication-row" id="lee2020neural">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">NeurIPS</abbr>
     
  </div>

  <div class="col-sm-10">
    
    <span class="title"><b>Neural Complexity Measures</b></span>
    <span class="author">
         
      <b> Yoonho Lee</b>,         Juho Lee,          Sung Ju Hwang,          Eunho Yang,          Seungjin Choi     
    </span>

    <span class="periodical">
      
      <em>NeurIPS 2020</em>
      
    </span>
    

    <span class="links">
       
      [<a class="abstract publink">abstract</a>] 
       
        [<a class="publink" href="https://arxiv.org/abs/2008.02953" target="_blank">paper</a>]   [<a class="publink" href="/blog/2020/neural-complexity-measures/" target="_blank">blog</a>]   [<a class="publink" href="https://crossminds.ai/video/5fce4f33d78d8048ceb6e3c5/" target="_blank">video</a>]       [<a class="publink" href="https://github.com/yoonholee/neural-complexity" target="_blank">code</a>] 
       
      [<a class="bib publink">bibtex</a>] 
       
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: A meta-learning framework for predicting generalization.</strong> </p>
      <p>While various complexity measures for deep neural networks exist, specifying an appropriate measure capable of predicting and explaining generalization in deep networks has proven challenging. We propose Neural Complexity (NC), a meta-learning framework for predicting generalization. Our model learns a scalar complexity measure through interactions with many heterogeneous tasks in a data-driven way. The trained NC model can be added to the standard training loss to regularize any task learner in a standard supervised learning scenario. We contrast NC’s approach against existing manually-designed complexity measures and other meta-learning models, and we validate NC’s performance on multiple regression and classification tasks.</p>
    </span>
     
    
    <span class="bib hidden">
      <pre>@inproceedings{lee2020nc,
  title     = {Neural Complexity Measures},
  author    = {Yoonho Lee and Juho Lee and Sung Ju Hwang and Eunho Yang and Seungjin Choi},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2020}
}</pre>
    </span>
    
  </div>
</div>
</li></ol>

  <div class="year">2019</div>
  <ol class="bibliography"><li><div class="row publication-row" id="lee2019discrete">
  <div class="col-sm-2 abbr">
    
  </div>

  <div class="col-sm-10">
    
    <span class="title"><b>Discrete Infomax Codes for Supervised Representation Learning</b></span>
    <span class="author">
         
      <b> Yoonho Lee</b>,         Wonjae Kim,          Wonpyo Park,          Seungjin Choi     
    </span>

    <span class="periodical">
      
      <em>arXiv:1905.11656</em>
      
    </span>
    

    <span class="links">
       
      [<a class="abstract publink">abstract</a>] 
       
        [<a class="publink" href="https://arxiv.org/abs/1905.11656" target="_blank">paper</a>]        
       
      [<a class="bib publink">bibtex</a>] 
       
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: Regularizing few-shot classification using compact discrete codes.</strong> </p>
      <p>Learning compact discrete representations of data is a key task on its own or for facilitating subsequent processing of data. In this paper we present a model that produces Discrete InfoMax COdes (DIMCO); we learn a probabilistic encoder that yields k-way d-dimensional codes associated with input data. Our model’s learning objective is to maximize the mutual information between codes and labels with a regularization, which enforces entries of a codeword to be as independent as possible. We show that the infomax principle also justiﬁes previous loss functions (e.g., cross-entropy) as its special cases. Our analysis also shows that using shorter codes, as DIMCO does, reduces overﬁtting in the conext of few-shot classiﬁcation. Through experiments in various domains, we observe this implicit meta-regularization effect of DIMCO. Furthermore, we show that the codes learned by DIMCO are efﬁcient in terms of both memory and retrieval time compared to previous methods.</p>
    </span>
     
    
    <span class="bib hidden">
      <pre>@article{lee2019dimco,
  title         = {Discrete Infomax Codes for Supervised Representation Learning},
  author        = {Yoonho Lee and Wonjae Kim and Wonpyo Park and Seungjin Choi},
  year          = {2019},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  eprint        = {1905.11656}
}</pre>
    </span>
    
  </div>
</div>
</li>
<li><div class="row publication-row" id="lee2019deep">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">NeurIPS-W</abbr>
     
  </div>

  <div class="col-sm-10">
    
    <span class="title"><b>Deep Amortized Clustering</b></span>
    <span class="author">
           Juho Lee,        
      <b> Yoonho Lee</b>,         Yee Whye Teh     
    </span>

    <span class="periodical">
      
      <em>Sets and Parts Workshop @ NeurIPS 2019 (oral)</em>
      
    </span>
    

    <span class="links">
       
      [<a class="abstract publink">abstract</a>] 
       
        [<a class="publink" href="https://arxiv.org/abs/1909.13433" target="_blank">paper</a>]        
       
      [<a class="bib publink">bibtex</a>] 
       
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: Learning to cluster by identifying one cluster at a time.</strong> </p>
      <p>We propose Deep Amortized Clustering (DAC), a framework in which a neural network learns to cluster datasets efficiently using a few forward passes through a deep neural network. DAC implicitly learns what makes a cluster, how to group data points into clusters, and how to count the number of clusters in datasets. DAC is meta-learned in a data-driven way, using only clustered datasets and their partitions. This framework differs from traditional clustering algorithms, which usually require user-specified prior knowledge about the shape or structure of clusters. We empirically show on both synthetic and image data that DAC can efficiently and accurately cluster novel datasets.</p>
    </span>
     
    
    <span class="bib hidden">
      <pre>@article{lee2019dac,
  title     = {Deep Amortized Clustering},
  author    = {Juho Lee and Yoonho Lee and Yee Whye Teh},
  booktitle = {Sets and Parts Workshop @ NeurIPS},
  year      = {2019}
}</pre>
    </span>
    
  </div>
</div>
</li>
<li><div class="row publication-row" id="kim2019learning">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">NeurIPS</abbr>
     
  </div>

  <div class="col-sm-10">
    
    <span class="title"><b>Learning Dynamics of Attention: Human Prior for Interpretable Machine Reasoning</b></span>
    <span class="author">
           Wonjae Kim,        
      <b> Yoonho Lee</b>
         
    </span>

    <span class="periodical">
      
      <em>NeurIPS 2019</em>
      
    </span>
    

    <span class="links">
       
      [<a class="abstract publink">abstract</a>] 
       
        [<a class="publink" href="https://arxiv.org/abs/1905.11666" target="_blank">paper</a>]         [<a class="publink" href="https://github.com/kakao/DAFT" target="_blank">code</a>] 
       
      [<a class="bib publink">bibtex</a>] 
       
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: Smooth and interpretable attention using Neural ODEs.</strong> </p>
      <p>Without relevant human priors, neural networks may learn uninterpretable features. We propose Dynamics of Attention for Focus Transition (DAFT) as a human prior for machine reasoning. DAFT is a novel method that regularizes attention-based reasoning by modelling it as a continuous dynamical system using neural ordinary differential equations. As a proof of concept, we augment a state-of-the-art visual reasoning model with DAFT. Our experiments reveal that applying DAFT yields similar performance to the original model while using fewer reasoning steps, showing that it implicitly learns to skip unnecessary steps. We also propose a new metric, Total Length of Transition (TLT), which represents the effective reasoning step size by quantifying how much a given model’s focus drifts while reasoning about a question. We show that adding DAFT results in lower TLT, demonstrating that our method indeed obeys the human prior towards shorter reasoning paths in addition to producing more interpretable attention maps.</p>
    </span>
     
    
    <span class="bib hidden">
      <pre>@inproceedings{kim2019daft,
  title     = {Learning Dynamics of Attention: Human Prior for Interpretable Machine Reasoning},
  author    = {Kim, Wonjae and Yoonho Lee},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2019}
}</pre>
    </span>
    
  </div>
</div>
</li>
<li><div class="row publication-row" id="pmlr-v97-lee19d">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">ICML</abbr>
     
  </div>

  <div class="col-sm-10">
    
    <span class="title"><b>Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks</b></span>
    <span class="author">
           Juho Lee,        
      <b> Yoonho Lee</b>,         Jungtaek Kim,          Adam Kosiorek,          Seungjin Choi,          Yee Whye Teh     
    </span>

    <span class="periodical">
      
      <em>ICML 2019</em>
      
    </span>
    

    <span class="links">
       
      [<a class="abstract publink">abstract</a>] 
       
        [<a class="publink" href="http://proceedings.mlr.press/v97/lee19d/lee19d.pdf" target="_blank">paper</a>]         [<a class="publink" href="https://github.com/juho-lee/set_transformer" target="_blank">code</a>] 
       
      [<a class="bib publink">bibtex</a>] 
       
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: Use Transformer blocks in your set networks.</strong> </p>
      <p>Many machine learning tasks such as multiple instance learning, 3D shape recognition, and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces the computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating the state-of-the-art performance compared to recent methods for set-structured data.</p>
    </span>
     
    
    <span class="bib hidden">
      <pre>@inproceedings{lee2019st,
  title     = {Set Transformer: A Framework for Attention-based Permutation-invariant Neural Networks},
  author    = {Juho Lee and Yoonho Lee and Jungtaek Kim and Adam Kosiorek and Seungjin Choi and Yee Whye Teh},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2019}
}</pre>
    </span>
    
  </div>
</div>
</li></ol>

  <div class="year">2018</div>
  <ol class="bibliography"><li><div class="row publication-row" id="lee2018gradient">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">ICML</abbr>
     
  </div>

  <div class="col-sm-10">
    
    <span class="title"><b>Gradient-based Meta-learning with Learned Layerwise Metric and Subspace</b></span>
    <span class="author">
         
      <b> Yoonho Lee</b>,         Seungjin Choi     
    </span>

    <span class="periodical">
      
      <em>ICML 2018</em>
      
    </span>
    

    <span class="links">
       
      [<a class="abstract publink">abstract</a>] 
       
        [<a class="publink" href="http://proceedings.mlr.press/v80/lee18a/lee18a.pdf" target="_blank">paper</a>]    [<a class="publink" href="https://youtu.be/skcErc5DBYM?t=2904" target="_blank">video</a>]       [<a class="publink" href="https://github.com/yoonholee/MT-net" target="_blank">code</a>] 
       
      [<a class="bib publink">bibtex</a>] 
       
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: We architecturally separate task-general and task-specific learning in MAML.</strong> </p>
      <p>
    Gradient-based meta-learning methods leverage gradient descent to learn the commonalities among various tasks. 
    While previous such methods have been successful in meta-learning tasks, they resort to simple gradient descent during meta-testing. 
    Our primary contribution is the MT-net, which enables the meta-learner to learn on each layer’s activation space a subspace that the task-specific learner performs gradient descent on. 
    Additionally, a task-specific learner of an MT-net performs gradient descent with respect to a meta-learned distance metric, which warps the activation space to be more sensitive to task identity. 
    We demonstrate that the dimension of this learned subspace reflects the complexity of the task-specific learner’s adaptation task, and also that our model is less sensitive to the choice of initial learning rates than previous gradient-based meta-learning methods. 
    Our method achieves state-of-the-art or comparable performance on few-shot classification and regression tasks.</p>
    </span>
     
    
    <span class="bib hidden">
      <pre>@inproceedings{lee2018mtnet,
  title     = {Gradient-based Meta-learning with Learned Layerwise Metric and Subspace},
  author    = {Yoonho Lee and Seungjin Choi},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2018}
}</pre>
    </span>
    
  </div>
</div>
</li></ol>


</div>

  </article>

</div>

    </div>
    </div>

  <!-- Footer -->
  <!-- Twitter universal website tag code -->
<script>
!function(e,t,n,s,u,a){e.twq||(s=e.twq=function(){s.exe?s.exe.apply(s,arguments):s.queue.push(arguments);
},s.version='1.1',s.queue=[],u=t.createElement(n),u.async=!0,u.src='//static.ads-twitter.com/uwt.js',
a=t.getElementsByTagName(n)[0],a.parentNode.insertBefore(u,a))}(window,document,'script');
// Insert Twitter Pixel ID and Standard Event data below
twq('init','o54b6');
twq('track','PageView');
</script>
<!-- End Twitter universal website tag code -->

<div id="footer" class="social">
  <span class="contact-icon text-center">
  <!-- <h4>Contact Info</h4> -->
  <h4>Yoonho Lee</h4>
  <a href="mailto:%79%6F%6F%6E%68%6F%6C%65%65%39%35@%67%6D%61%69%6C.%63%6F%6D" name="email link">
    📩 Email</a>
  <span class="contact-separator"> · </span>
  <a rel="noopener" href="https://scholar.google.com/citations?user=BAAZ_ysAAAAJ" target="_blank" title="Google Scholar">
    📖 Google Scholar</a>
  <span class="contact-separator"> · </span>
  <a rel="noopener" href="https://github.com/yoonholee" target="_blank" title="GitHub">
    🐙 Github</a>
  <span class="contact-separator"> · </span>
  <a rel="noopener" href="https://twitter.com/yoonholeee" target="_blank" title="Twitter">
    🐦 Twitter</a>
<!-- 
  <a href="mailto:%79%6F%6F%6E%68%6F%6C%65%65%39%35@%67%6D%61%69%6C.%63%6F%6D" name="email link"><i class="fas fa-envelope"></i></a>
  <a rel="noopener" href="https://scholar.google.com/citations?user=BAAZ_ysAAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
  <a rel="noopener" href="https://github.com/yoonholee" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
  
  <a rel="noopener" href="https://twitter.com/yoonholeee" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a> -->
</span>

</div>

  </body>

  <!-- jQuery -->
<script
  src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"
  integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg=="
  crossorigin="anonymous"
></script>

  <!-- Bootsrap & MDB scripts -->
<script
  src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js"
  integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A=="
  crossorigin="anonymous"
></script>
<script
  src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"
  integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ=="
  crossorigin="anonymous"
></script>
<!-- <script
  src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js"
  integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw=="
  crossorigin="anonymous"
></script> -->

  
<!-- Mansory & imagesLoaded -->
<script
  defer
  src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"
></script>
<script
  defer
  src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"
></script>
<script
  defer
  src="/assets/js/mansory.js"
  type="text/javascript"
></script>


  <script src="/assets/js/common.js"></script>

 


<!-- Global site tag (gtag.js) - Google Analytics 
https://developers.google.com/analytics/devguides/collection/gtagjs
-->

<script
  async
  src="https://www.googletagmanager.com/gtag/js?id=UA-56028200-1"
></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());

  gtag("config", "UA-56028200-1");
</script>



</html>
