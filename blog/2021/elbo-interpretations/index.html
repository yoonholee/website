<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Yoonho  Lee | Better Interpretations of the ELBO</title>
<meta name="description" content="Personal website
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link defer rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link defer rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link defer rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">

<!-- Styles -->
<link defer rel="stylesheet" href="/assets/css/main.css">
<link rel="shortcut icon" href="/assets/img/favicon.ico">

<!-- Font -->
<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@300;400;600;700&display=swap" rel="stylesheet">

<link rel="canonical" href="https://yoonholee.com/"> 
<!-- MathJax -->
<script
  defer
  type="text/javascript"
  id="MathJax-script"
  src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"
></script>
<script
  defer
  src="https://polyfill.io/v3/polyfill.min.js?features=es6"
></script>


    <script defer src="/assets/js/distillpub/template.v2.js"></script>
    <script defer src="/assets/js/distillpub/transforms.v2.js"></script>
    
    <style type="text/css">
      .my-img {
  text-align: center;
  padding: 1.5rem 0rem;
}
d-article p, d-article ul, d-article ol, d-article blockquote {
  margin: 0.5rem 0;
}

    </style>
    
  </head>

  <d-front-matter>
    <script async type="text/json">
      {
            "title": "Better Interpretations of the ELBO",
            "description": "A generalized KL divergence, Helmholtz free energy, and bits-back coding",
            "published": "2021-04-22 00:00:00 +0900",
            "authors": [
              
              {
                "author": "Yoonho Lee",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script>
  </d-front-matter>

  <body
    class=" sticky-bottom-footer"
  >
    <!-- Header -->

    <header>
  <!-- Nav Bar -->
  <nav
    id="navbar"
    class="navbar navbar-light bg-white navbar-expand-sm sticky-top"
  >
    <div class="container">
      
      <a
        class="navbar-brand title:"
        href="https://yoonholee.com/"
        >Yoonho Lee</a
      >
      <!-- Navbar Toogle -->
      <button
        class="navbar-toggler collapsed ml-auto"
        type="button"
        data-toggle="collapse"
        data-target="#navbarNav"
        aria-controls="navbarNav"
        aria-expanded="false"
        aria-label="Toggle navigation"
      >
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About 
            </a>
          </li>
          <!-- Other pages -->
                  
          <li class="nav-item ">
            <a class="nav-link" href="/publications/">
              Publications 
            </a>
          </li>
               
          
          <!-- Blog -->
          <li
            class="nav-item active"
          >
            <a class="nav-link" href="/blog/">
              Blog 
            </a>
          </li>
          
          <li class="nav-item">
            <a class="nav-link" href="/assets/pdf/CV.pdf">CV </a>
          </li>
        </ul>
      </div>
    </div>
  </nav>
</header>


    <!-- Content -->

    <div class="post distill">
      <d-title>
        <h1>Better Interpretations of the ELBO</h1>
        <p>A generalized KL divergence, Helmholtz free energy, and bits-back coding</p>
      </d-title>

      <d-byline></d-byline>

      <d-article> <p>Variational Bayesian methods optimize a quantity called the Evidence Lower BOund (ELBO).
I think the standard way of motivating and deriving this objective fails to capture what it is and why we should care.</p>

<p>Here is the standard setup and derivation of the ELBO.
Unobserved variables \(z\) generate observed data \(x\) according to \(p(x|z)\).
We use a distribution \(q(z)\) to approximate the posterior \(p(z|x) \propto p(z)p(x|z)\).
We can get a lower bound of the evidence using Jensen’s inequality:</p>

\[\begin{align}
\log p(x) &amp;= \log \mathbb{E}_{z \sim q(z)} \left[\frac{p(x, z)}{q(z)} \right] \\
&amp;\geq \mathbb{E}_{z \sim q(z)} \left[ \log \frac{p(x, z)}{q(z)} \right] \triangleq \mathrm{ELBO}.
\end{align}\]

<p>A natural question is: why is this called “the” evidence lower bound?
There are infinitely many quantities that lower bound the evidence.
What is the significance of this one?</p>

<p>In this post, I will motivate the ELBO from three different viewpoints. 
The ELBO directly arises as a quantity of interest in each interpretation, better motivating its use as an objective.</p>

<h3 id="kl-divergence-for-unnormalized-densities">KL Divergence for Unnormalized Densities</h3>

<p>Let \(f(z)\) be an unnormalized density function, meaning that \(f(z) \geq 0\) everywhere and \(0 &lt; \int f(z) dz &lt; \infty\).
Normalizing this function to integrate to \(1\) would result in a probability density function:</p>

\[p_f(z) 
\triangleq \frac{f(z)}{\int f(z) dz}
= \frac{f(z)}{Z}.\]

<p>Note that the KL divergence between \(q(z)\) and \(p_f(z)\) can be expressed using \(q(z)\) and \(f(z)\):</p>

\[\begin{align}
\mathrm{KL} \left( q(z) \rVert p_f(z) \right)
&amp;= \mathbb{E}_{z \sim q(z)} \left[ \log q(z) - \log p_f(z) \right] \\
&amp;= \mathbb{E}_{z \sim q(z)} \left[ \log q(z) - \log f(z) \right] + Z \\
&amp;\triangleq \mathrm{KL}^* \left( q(z) \rVert f(z) \right) + Z.
\end{align}\]

<p>The constant \(Z\) can be ignored if we want to minimize this w.r.t. the distribution \(q(z)\).
Note that we have introduced the new symbol \(\mathrm{KL}^* \left( \cdot \rVert \cdot \right)\) to signify the equation’s similarity to the KL divergence.
This can be seen as a direct generalization of the KL divergence which measures the similarity between any distribution \(q(z)\) and nonnegative function \(f(z)\). 
\(\mathrm{KL}^* \left( q(z) \rVert f(z) \right)\) has minimum value when \(q(z) \propto f(z)\), just as \(\mathrm{KL} \left( q(z) \rVert p_f(z) \right)\) has minimum value when \(q(z)=p_f(z)\).</p>

<p>The ELBO is just a special case of this similarity measure where \(f_p(z) = p(x,z)\):</p>

\[\begin{align}
-\mathrm{ELBO} 
= \mathbb{E}_{z \sim q(z)} \left[ \log q(z) - \log p(x, z) \right]
= \mathrm{KL}^* \left( q(z) \rVert f_p(z) \right).
\end{align}\]

<h3 id="free-energy">Free Energy</h3>

<p>Think of the possible values of \(z\) as the phase space of a physical system.
We assume \(x\) is observed and that \((z, x) \sim p(z)p(x|z)\).
The statistical description of \(z\) through Bayes rule is that \(p(z|x) \propto p(z)p(x|z)\).
Equivalently, we can say that \(z\) follows the Boltzmann distribution for energy function <d-footnote>
For simplicity, let inverse temperature be 1.</d-footnote>.</p>

\[E(z) \triangleq -\log p(z) - \log p(x|z).\]

<p>A fundamental result from statistical mechanics: given energy function \(E\), the <em>free energy</em> defined as</p>

\[F(q(z)) \triangleq \mathbb{E}_{q(z)} \left[ E(z) \right] - H(q(z))\]

<p>is minimized when \(q(z)\) is the Boltzmann distribution w.r.t. \(E\) <d-footnote>
Given fixed total energy. We have implicitly set the total energy level in our previous choice of temperature.</d-footnote>.
Therefore, adjusting \(q(z)\) to minimize the free energy is equivalent to making \(q(z)\) follow the Boltzmann distribution. 
We rearrange to recover the ELBO:</p>

\[\begin{align}
F(q(z)) = \mathbb{E}_{q(z)} \left[ -\log p(z) -\log p(x|z) + \log q(z) \right] = -\mathrm{ELBO}.
\end{align}\]

<p>This interpretation of the ELBO has its roots in the concept of Helmholtz free energy in thermodynamics.
Free energy has since been shown to emerge from the principle of maximum entropy<d-cite key="jaynes1957information"></d-cite>, independently of thermodynamics.
More recently, the <em>free energy principle</em><d-cite key="fristonFreeenergyPrincipleUnified2010a"></d-cite> casts a wide array of empirical observations of lifelike or intelligent behavior as free energy minimization.</p>

<h3 id="description-length">Description Length</h3>

<p>We invoke a basic lemma from information theory.
Assume that a sender and receiver agree on a distribution \(p_1(x)\) and constructed an optimal code for it.
The expected description length of a sample from \(x \sim p_2(x)\) is</p>

\[\mathbb{E}_{x \sim p_2(x)} [-\log p_1(x)].\]

<p>We construct a coding scheme for transmitting observations \(x\) generated from unobserved latent variables \(z\).
The sender and receiver share knowledge of the generative distribution \(p(x|z)\) along with an approximate posterior \(q(z|x)\).
The sender uses the following coding scheme:</p>
<ul>
  <li>Observe \(x\) from data, sample \(z \sim p(z \vert x)\)</li>
  <li>Encode \(z\) according to \(p(z)\)</li>
  <li>Encode \(x\) according to \(p(x \vert z)\)</li>
  <li>Send encoded \(z, x\)</li>
</ul>

<p>Naively applying the lemma above to the two parts of the code, it may seem that the expected description length for this two-part code is</p>

\[\mathbb{E}_{x \sim \textrm{data}, z \sim q(z \vert x)} \left[ -\log p(z) -\log p(x|z) \right].\]

<p>The <strong>Bits-back argument</strong> states that one can get a “refund” from this code because of duplicate information in \(z\) and \(x\).
The two-part code is redundant because the sender’s inference model \(q(z|x)\) is also known by the receiver.
This redundancy is the number of bits one can learn about \(z\) from \(x\) using \(q(z|x)\), which is \(-\log q(z|x)\) bits.
Therefore, the true expected description length is</p>

\[\mathbb{E}_{x \sim \textrm{data}, z \sim q(z \vert x)} \left[ -\log p(z) -\log p(x|z) + \log q(z|x) \right] = -\mathrm{ELBO},\]

<p>so maximizing the ELBO can be seen as minimizing the expected code length of this protocol.</p>

<p>The bits-back argument was first introduced by Hinton and van Camp<d-cite key="hinton1993keeping"></d-cite> in the context of minimizing the description length of neural network weights.
More recently, Chen et al<d-cite key="vlae"></d-cite> use the bits-back coding interpretation of the variational autoencoder (VAE) to explain its failure modes,
and Townsend et al<d-cite key="bitsbackdeep"></d-cite> implement an efficient lossless bits-back coding scheme using a VAE.</p>
 </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>
    </div>

    <!-- Footer -->

    <!-- Twitter universal website tag code -->
<script>
!function(e,t,n,s,u,a){e.twq||(s=e.twq=function(){s.exe?s.exe.apply(s,arguments):s.queue.push(arguments);
},s.version='1.1',s.queue=[],u=t.createElement(n),u.async=!0,u.src='//static.ads-twitter.com/uwt.js',
a=t.getElementsByTagName(n)[0],a.parentNode.insertBefore(u,a))}(window,document,'script');
// Insert Twitter Pixel ID and Standard Event data below
twq('init','o54b6');
twq('track','PageView');
</script>
<!-- End Twitter universal website tag code -->

<div id="footer" class="social">
  <span class="contact-icon text-center">
  <!-- <h4>Contact Info</h4> -->
  <h4>Yoonho Lee</h4>
  <a href="mailto:%79%6F%6F%6E%68%6F%6C%65%65%39%35@%67%6D%61%69%6C.%63%6F%6D" name="email link">
    📩 Email</a>
  <span class="contact-separator"> · </span>
  <a rel="noopener" href="https://scholar.google.com/citations?user=BAAZ_ysAAAAJ" target="_blank" title="Google Scholar">
    📖 Google Scholar</a>
  <span class="contact-separator"> · </span>
  <a rel="noopener" href="https://github.com/yoonholee" target="_blank" title="GitHub">
    🐙 Github</a>
  <span class="contact-separator"> · </span>
  <a rel="noopener" href="https://twitter.com/yoonholeee" target="_blank" title="Twitter">
    🐦 Twitter</a>
<!-- 
  <a href="mailto:%79%6F%6F%6E%68%6F%6C%65%65%39%35@%67%6D%61%69%6C.%63%6F%6D" name="email link"><i class="fas fa-envelope"></i></a>
  <a rel="noopener" href="https://scholar.google.com/citations?user=BAAZ_ysAAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
  <a rel="noopener" href="https://github.com/yoonholee" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
  
  <a rel="noopener" href="https://twitter.com/yoonholeee" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a> -->
</span>

</div>
  </body>

  <d-bibliography
    src="/assets/bibliography/2018-12-22-distill.bib"
  >
  </d-bibliography>
</html>
