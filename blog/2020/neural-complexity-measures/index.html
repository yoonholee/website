<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Yoonho  Lee | Neural Complexity Measures</title>
<meta name="description" content="Personal website
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link defer rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link defer rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link defer rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">

<!-- Styles -->
<link defer rel="stylesheet" href="/assets/css/main.css">
<link rel="shortcut icon" href="/assets/img/favicon.ico">

<!-- Font -->
<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@300;400;600;700&display=swap" rel="stylesheet">

<link rel="canonical" href="https://yoonholee.com/"> 
<!-- MathJax -->
<script
  defer
  type="text/javascript"
  id="MathJax-script"
  src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"
></script>
<script
  defer
  src="https://polyfill.io/v3/polyfill.min.js?features=es6"
></script>


    <script defer src="/assets/js/distillpub/template.v2.js"></script>
    <script defer src="/assets/js/distillpub/transforms.v2.js"></script>
    
    <style type="text/css">
      .my-img {
  text-align: center;
  padding: 1.5rem 0rem;
}

    </style>
    
  </head>

  <d-front-matter>
    <script async type="text/json">
      {
            "title": "Neural Complexity Measures",
            "description": "A meta-learning framework for predicting generalization",
            "published": "2020-12-01 00:00:00 +0900",
            "authors": [
              
              {
                "author": "Yoonho Lee",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script>
  </d-front-matter>

  <body
    class=" sticky-bottom-footer"
  >
    <!-- Header -->

    <header>
  <!-- Nav Bar -->
  <nav
    id="navbar"
    class="navbar navbar-light bg-white navbar-expand-sm sticky-top"
  >
    <div class="container">
      
      <a
        class="navbar-brand title:"
        href="https://yoonholee.com/"
        >Yoonho Lee</a
      >
      <!-- Navbar Toogle -->
      <button
        class="navbar-toggler collapsed ml-auto"
        type="button"
        data-toggle="collapse"
        data-target="#navbarNav"
        aria-controls="navbarNav"
        aria-expanded="false"
        aria-label="Toggle navigation"
      >
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About 
            </a>
          </li>
          <!-- Other pages -->
                  
          <li class="nav-item ">
            <a class="nav-link" href="/publications/">
              Publications 
            </a>
          </li>
               
          
          <!-- Blog -->
          <li
            class="nav-item active"
          >
            <a class="nav-link" href="/blog/">
              Blog 
            </a>
          </li>
          
          <li class="nav-item">
            <a class="nav-link" href="/assets/pdf/CV.pdf">CV </a>
          </li>
        </ul>
      </div>
    </div>
  </nav>
</header>


    <!-- Content -->

    <div class="post distill">
      <d-title>
        <h1>Neural Complexity Measures</h1>
        <p>A meta-learning framework for predicting generalization</p>
      </d-title>

      <d-byline></d-byline>

      <d-article> <p>This post is a summary of the paper <a href="/publications/#lee2020neural">Neural Complexity Measures</a>.</p>

<p>Say we have a neural network \(f_\theta: \mathcal{X} \rightarrow \mathcal{Y}\).
We want to predict and/or prevent overfitting, so we are often interested in measuring the complexity of the function \(f_\theta\).
<strong>How should we measure the complexity of a neural network?</strong>
Some common approaches are:</p>

<ul>
  <li>number of parameters in \(\theta\)</li>
  <li>norm of parameter vector \(\Vert \theta \Vert_p\)</li>
  <li>distance to initialization \(\Vert \theta - \theta_0 \Vert_p\)</li>
  <li>flatness of minima \(\Vert \nabla^2 \mathcal{L} (\theta) \Vert\)</li>
</ul>

<p>Each of these hand-designed measures of complexity fails to capture the behavior of neural networks used in practice. 
A common feature, attributable to the fact that people designed them, is that they‚Äôre simple equations defined in parameter space. 
I believe that defining a complexity measure in parameter space is the wrong approach. 
Parameter space is insanely messy and high-dimensional, and a simple equation like the ones above will likely be insufficient to capture this intricacy. 
Another concern is in calibration between models: changing the architecture or increasing the number of parameters alters the parameter space‚Äôs geometry drastically, making a comparison between different models hard.</p>

<h2 id="neural-complexity-measures">Neural Complexity Measures</h2>
<p>We <d-cite key="lee2020nc"></d-cite> took a different approach:
(1) <strong>we defined a complexity measure in function space</strong>, and (2) <strong>we learned this measure in a data-driven way</strong>.
More concretely, for any given neural network, a meta-learned model predicts its generalization gap:</p>
<div class="my-img l-body" style="padding-left: 10%; padding-right: 10%;">
  

<img src="/assets/img/201201_nc_gap.jpg" srcset="            /assets/resized/201201_nc_gap-320x117.jpg 320w,            /assets/resized/201201_nc_gap-480x175.jpg 480w,            /assets/resized/201201_nc_gap-800x292.jpg 800w,     /assets/img/201201_nc_gap.jpg 1920w" width="100%" alt="Definition of the generalization gap." /> 
</div>

<p>The generalization gap is a direct quantitative measure of the degree of overfitting. 
While most approaches attempt to find suitable proxies for this quantity, we adopt a meta-learning framework that treats the estimation of the generalization gap as a set-input regression problem.</p>

<p>We call this meta-learned estimator a Neural Complexity (NC) measure. 
We train NC with the following meta-learning loop:</p>
<div class="my-img l-body-outset">
  

<img src="/assets/img/201201_nc_loop.jpg" srcset="            /assets/resized/201201_nc_loop-320x106.jpg 320w,            /assets/resized/201201_nc_loop-480x160.jpg 480w,            /assets/resized/201201_nc_loop-800x266.jpg 800w,     /assets/img/201201_nc_loop.jpg 4778w" width="100%" alt="Training loop of the neural complexity measures model." /> 
</div>
<p>We continually use NC as a regularizer for new task learning runs and store snapshots of these runs into a memory bank. 
We train NC using minibatches of snapshots, sampled randomly from the memory bank.</p>

<p>NC is a neural network which takes (data, outputs, labels) for training data and (data, outputs) for held-out validation data to produce a single scalar value:</p>
<div class="my-img l-body" style="padding-left: 10%; padding-right: 10%;">
  

<img src="/assets/img/201201_nc_architecture.jpg" srcset="            /assets/resized/201201_nc_architecture-320x161.jpg 320w,            /assets/resized/201201_nc_architecture-480x241.jpg 480w,            /assets/resized/201201_nc_architecture-800x402.jpg 800w,     /assets/img/201201_nc_architecture.jpg 1920w" width="100%" alt="Architecture of the neural complexity measures model." /> 
</div>

<p>In the paper, we show proof-of-concept experiments that show that an NC model can learn to predict the generalization gap in synthetic regression and real-world image classification problems. 
The trained NC models show signs of generalization to out-of-distribution learner architectures. 
Interestingly, using NC as a regularizer resulted in lower test loss than train loss.</p>

<h2 id="conclusion">Conclusion</h2>
<p>Neural Complexity (NC) is a meta-learning framework for predicting generalization, 
which has exciting implications for both meta-learning and understanding generalization in deep networks.</p>
<h3 id="meta-learning">Meta-Learning</h3>
<p>We typically use meta-learning for small problems such as few-shot classification. 
Because they rely on feeding the entire dataset into the computation graph, previous meta-learning frameworks<d-cite key="finn2017model,garnelo2018neural"></d-cite> are not scalable to massive datasets. 
NC is a much more ‚Äúlocal‚Äù meta-learning framework: a trained NC model acts like any other regularizer and runs alongside minibatch SGD. 
In the paper, we show that NC can successfully regularize relatively large tasks like CIFAR-10.
Further improvements may make NC a viable regularizer in ImageNet-scale tasks.</p>
<h3 id="generalization-in-deep-networks">Generalization in Deep Networks</h3>
<p>This work showed that a meta-learned model could predict the generalization gap reliably enough to be used as a regularizer. We also proposed a simple way of translating regression accuracy to a generalization bound.
I think this shows the potential of meta-learning as a tool for understanding generalization in deep networks.
Further improvements to the NC framework and integration with theoretical tools for understanding generalization could be a promising way forward in this problem.</p>
 </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>
    </div>

    <!-- Footer -->

    <!-- Twitter universal website tag code -->
<script>
!function(e,t,n,s,u,a){e.twq||(s=e.twq=function(){s.exe?s.exe.apply(s,arguments):s.queue.push(arguments);
},s.version='1.1',s.queue=[],u=t.createElement(n),u.async=!0,u.src='//static.ads-twitter.com/uwt.js',
a=t.getElementsByTagName(n)[0],a.parentNode.insertBefore(u,a))}(window,document,'script');
// Insert Twitter Pixel ID and Standard Event data below
twq('init','o54b6');
twq('track','PageView');
</script>
<!-- End Twitter universal website tag code -->

<div id="footer" class="social">
  <span class="contact-icon text-center">
  <!-- <h4>Contact Info</h4> -->
  <h4>Yoonho Lee</h4>
  <a href="mailto:%79%6F%6F%6E%68%6F%6C%65%65%39%35@%67%6D%61%69%6C.%63%6F%6D" name="email link">
    üì© Email</a>
  <span class="contact-separator"> ¬∑ </span>
  <a rel="noopener" href="https://scholar.google.com/citations?user=BAAZ_ysAAAAJ" target="_blank" title="Google Scholar">
    üìñ Google Scholar</a>
  <span class="contact-separator"> ¬∑ </span>
  <a rel="noopener" href="https://github.com/yoonholee" target="_blank" title="GitHub">
    üêô Github</a>
  <span class="contact-separator"> ¬∑ </span>
  <a rel="noopener" href="https://twitter.com/yoonholeee" target="_blank" title="Twitter">
    üê¶ Twitter</a>
<!-- 
  <a href="mailto:%79%6F%6F%6E%68%6F%6C%65%65%39%35@%67%6D%61%69%6C.%63%6F%6D" name="email link"><i class="fas fa-envelope"></i></a>
  <a rel="noopener" href="https://scholar.google.com/citations?user=BAAZ_ysAAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
  <a rel="noopener" href="https://github.com/yoonholee" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
  
  <a rel="noopener" href="https://twitter.com/yoonholeee" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a> -->
</span>

</div>
  </body>

  <d-bibliography
    src="/assets/bibliography/2018-12-22-distill.bib"
  >
  </d-bibliography>
</html>
